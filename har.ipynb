{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyOsmS+It0rGixDelBCDtzIJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zwVhau7pcsdx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729523321472,"user_tz":-420,"elapsed":3485,"user":{"displayName":"Phawinee Suphawimon","userId":"15622915544784522799"}},"outputId":"58e1d2f3-4c5d-4ec1-d89b-1d23294e6ac6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","\n","directory = '/content/drive/MyDrive/Colab Notebooks/ML_Kaggle2/ai-4-ba-ml-2024-har'\n","\n","files = os.listdir(directory)\n","\n","for file in files:\n","    print(file)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QoPKpTDhV7Xc","executionInfo":{"status":"ok","timestamp":1729523321473,"user_tz":-420,"elapsed":8,"user":{"displayName":"Phawinee Suphawimon","userId":"15622915544784522799"}},"outputId":"2ce94501-1d94-4ecb-d6fc-3d30c49829e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["demo.ipynb\n","sample_submission.csv\n","Train\n","Test\n","Accuracy.gdoc\n","Har.ipynb\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import f1_score, classification_report\n","import glob\n","import os\n","from imblearn.over_sampling import SMOTE\n","\n","TRAIN_DIR = directory + \"/Train/\"\n","TEST_DIR = directory + \"/Test/\"\n","\n","def load_data(folder_path):\n","    all_data = []\n","    all_labels = []\n","\n","    print(f\"Attempting to load data from: {folder_path}\")\n","\n","    if not os.path.exists(folder_path):\n","        raise ValueError(f\"The folder path {folder_path} does not exist.\")\n","\n","    for label in range(18):\n","        csv_files = glob.glob(os.path.join(folder_path, str(label), \"*.csv\"))\n","        print(f\"Found {len(csv_files)} CSV files for label {label}\")\n","\n","        for file in csv_files:\n","            try:\n","                df = pd.read_csv(file)\n","                features = extract_features(df)\n","                all_data.append(features)\n","                all_labels.append(label)\n","            except Exception as e:\n","                print(f\"Error processing file {file}: {str(e)}\")\n","\n","    if len(all_data) == 0:\n","        raise ValueError(\"No data was loaded. Please check the folder structure and file paths.\")\n","\n","    print(f\"Total samples loaded: {len(all_data)}\")\n","    return np.array(all_data), np.array(all_labels)\n","\n","def extract_features(df):\n","    features = []\n","\n","    # Basic statistical features\n","    for col in df.columns:\n","        features.extend([\n","            df[col].mean(),\n","            df[col].std(),\n","            df[col].min(),\n","            df[col].max(),\n","            df[col].median()\n","        ])\n","\n","    # Accelerometer features\n","    df['acc_magnitude'] = np.sqrt(df['Ax']**2 + df['Ay']**2 + df['Az']**2)\n","    features.extend([\n","        df['acc_magnitude'].mean(),\n","        df['acc_magnitude'].std(),\n","        df['acc_magnitude'].max()\n","    ])\n","\n","    df['avg_acc'] = df[['Ax', 'Ay', 'Az']].mean(axis=1)\n","    df['max_acc'] = df[['Ax', 'Ay', 'Az']].max(axis=1)\n","\n","    features.extend([\n","        df['avg_acc'].mean(),\n","        df['max_acc'].mean()\n","    ])\n","\n","    # Velocity features\n","    df['velocity_x'] = df['Ax'].cumsum()\n","    df['velocity_y'] = df['Ay'].cumsum()\n","    df['velocity_z'] = df['Az'].cumsum()\n","\n","    features.extend([\n","        df['velocity_x'].mean(),\n","        df['velocity_y'].mean(),\n","        df['velocity_z'].mean()\n","    ])\n","\n","    # Jerk features\n","    df['jerk_x'] = df['Ax'].diff()\n","    df['jerk_y'] = df['Ay'].diff()\n","    df['jerk_z'] = df['Az'].diff()\n","\n","    features.extend([\n","        df['jerk_x'].mean(),\n","        df['jerk_y'].mean(),\n","        df['jerk_z'].mean()\n","    ])\n","\n","    # Gyroscope features\n","    df['gyro_magnitude'] = np.sqrt(df['Gx']**2 + df['Gy']**2 + df['Gz']**2)\n","    features.extend([\n","        df['gyro_magnitude'].mean(),\n","        df['gyro_magnitude'].std(),\n","        df['gyro_magnitude'].max()\n","    ])\n","\n","    # Signal magnitude area (SMA)\n","    window_size = 5\n","    df['sma_ax'] = df['Ax'].rolling(window=window_size).mean()\n","    df['sma_ay'] = df['Ay'].rolling(window=window_size).mean()\n","    df['sma_az'] = df['Az'].rolling(window=window_size).mean()\n","\n","    features.extend([\n","        df['sma_ax'].mean(),\n","        df['sma_ay'].mean(),\n","        df['sma_az'].mean()\n","    ])\n","\n","    return features\n","\n","def train_model():\n","    # Load and preprocess data\n","    X, y = load_data(TRAIN_DIR)\n","    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=101, stratify=y)\n","    print(\"==================Train data===================\")\n","    print(np.isnan(X).sum())\n","    print(\"===============================================\")\n","\n","    smote = SMOTE()\n","    X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n","    X_train = X_resampled\n","    y_train = y_resampled\n","\n","    # Scale features\n","    scaler = StandardScaler()\n","    X_train_scaled = scaler.fit_transform(X_train)\n","    X_val_scaled = scaler.transform(X_val)\n","\n","    # Hyperparameter distribution for tuning\n","    param_dist = {\n","        'n_estimators': list(range(100, 1001, 100)),\n","        'max_depth': list(range(5, 51, 5)),\n","        'min_samples_split': list(range(2, 11)),\n","        'min_samples_leaf': list(range(1, 6)),\n","        'max_features': ['sqrt', 'log2', None],\n","        'criterion': ['gini', 'entropy'],\n","        'bootstrap': [True, False]\n","    }\n","\n","    # Initialize RandomForestClassifier\n","    model = RandomForestClassifier(random_state=101)\n","\n","    # Set up RandomizedSearchCV\n","    random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist,\n","                                       n_iter=50, cv=3, n_jobs=-1, verbose=1, random_state=101,\n","                                       scoring='f1_macro')\n","\n","    # Fit model with hyperparameter tuning\n","    random_search.fit(X_train_scaled, y_train)\n","\n","    # Get the best model and its hyperparameters\n","    best_model = random_search.best_estimator_\n","    print(f\"Best parameters: {random_search.best_params_}\")\n","\n","    # Evaluate best model\n","    y_pred = best_model.predict(X_val_scaled)\n","\n","    # Calculate F1 score\n","    f1_macro = f1_score(y_val, y_pred, average='macro')\n","    f1_weighted = f1_score(y_val, y_pred, average='weighted')\n","    f1_per_class = f1_score(y_val, y_pred, average=None)\n","\n","    print(f\"Validation F1 Score (Macro): {f1_macro}\")\n","    print(f\"Validation F1 Score (Weighted): {f1_weighted}\")\n","    print(\"\\nF1 Score per class:\")\n","    for i, f1 in enumerate(f1_per_class):\n","        print(f\"Class {i}: {f1}\")\n","\n","    print(\"\\nDetailed Classification Report:\")\n","    print(classification_report(y_val, y_pred))\n","\n","    return best_model, scaler\n","\n","def predict_test_data(model, scaler):\n","    test_files = glob.glob(os.path.join(TEST_DIR, \"*.csv\"))\n","    test_predictions = []\n","\n","    for file in test_files:\n","        df = pd.read_csv(file)\n","        features = extract_features(df)\n","        features_scaled = scaler.transform([features])\n","        prediction = model.predict(features_scaled)[0]\n","        test_predictions.append((os.path.basename(file), prediction))\n","\n","    return test_predictions\n","\n","def write_submission_file(predictions):\n","    submission_df = pd.DataFrame(predictions, columns=['id', 'label'])\n","    submission_df.to_csv('submission.csv', index=False)\n","\n","try:\n","    model, scaler = train_model()\n","    test_predictions = predict_test_data(model, scaler)\n","    write_submission_file(test_predictions)\n","except Exception as e:\n","    print(f\"An error occurred: {str(e)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WAeOLjutmP5g","executionInfo":{"status":"ok","timestamp":1729529852521,"user_tz":-420,"elapsed":6531054,"user":{"displayName":"Phawinee Suphawimon","userId":"15622915544784522799"}},"outputId":"2b0d5f92-3e43-4a05-9e19-d0b7098c6bb6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Attempting to load data from: /content/drive/MyDrive/Colab Notebooks/ML_Kaggle2/ai-4-ba-ml-2024-har/Train/\n","Found 1500 CSV files for label 0\n","Found 1500 CSV files for label 1\n","Found 1200 CSV files for label 2\n","Found 1000 CSV files for label 3\n","Found 1500 CSV files for label 4\n","Found 80 CSV files for label 5\n","Found 90 CSV files for label 6\n","Found 70 CSV files for label 7\n","Found 60 CSV files for label 8\n","Found 100 CSV files for label 9\n","Found 70 CSV files for label 10\n","Found 500 CSV files for label 11\n","Found 60 CSV files for label 12\n","Found 70 CSV files for label 13\n","Found 60 CSV files for label 14\n","Found 60 CSV files for label 15\n","Found 60 CSV files for label 16\n","Found 60 CSV files for label 17\n","Total samples loaded: 8040\n","==================Train data===================\n","0\n","===============================================\n","Fitting 3 folds for each of 50 candidates, totalling 150 fits\n","Best parameters: {'n_estimators': 1000, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 25, 'criterion': 'entropy', 'bootstrap': False}\n","Validation F1 Score (Macro): 0.8736517868630231\n","Validation F1 Score (Weighted): 0.9350138818190621\n","\n","F1 Score per class:\n","Class 0: 0.9210950080515298\n","Class 1: 0.9399656946826758\n","Class 2: 0.9144050104384134\n","Class 3: 0.9728395061728395\n","Class 4: 0.9802631578947368\n","Class 5: 0.7857142857142857\n","Class 6: 0.9444444444444444\n","Class 7: 0.9230769230769231\n","Class 8: 0.9565217391304348\n","Class 9: 0.9743589743589743\n","Class 10: 0.6086956521739131\n","Class 11: 0.95\n","Class 12: 0.782608695652174\n","Class 13: 0.9230769230769231\n","Class 14: 0.9230769230769231\n","Class 15: 0.6666666666666666\n","Class 16: 0.7407407407407407\n","Class 17: 0.8181818181818182\n","\n","Detailed Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.95      0.92       300\n","           1       0.97      0.91      0.94       300\n","           2       0.92      0.91      0.91       240\n","           3       0.96      0.98      0.97       200\n","           4       0.97      0.99      0.98       300\n","           5       0.92      0.69      0.79        16\n","           6       0.94      0.94      0.94        18\n","           7       1.00      0.86      0.92        14\n","           8       1.00      0.92      0.96        12\n","           9       1.00      0.95      0.97        20\n","          10       0.78      0.50      0.61        14\n","          11       0.95      0.95      0.95       100\n","          12       0.82      0.75      0.78        12\n","          13       1.00      0.86      0.92        14\n","          14       0.86      1.00      0.92        12\n","          15       0.78      0.58      0.67        12\n","          16       0.67      0.83      0.74        12\n","          17       0.90      0.75      0.82        12\n","\n","    accuracy                           0.94      1608\n","   macro avg       0.91      0.85      0.87      1608\n","weighted avg       0.94      0.94      0.94      1608\n","\n"]}]},{"cell_type":"code","source":["write_submission_file(test_predictions)"],"metadata":{"id":"PJ-xcPT7XcL3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3cWr4pj0uMUF"},"execution_count":null,"outputs":[]}]}